#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import sys
import os
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from media_platform.bilibili import BilibiliCrawler
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥BilibiliCrawlerï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    # æ¨¡æ‹Ÿçˆ¬è™«ç±»ç”¨äºæµ‹è¯•
    class MockBilibiliCrawler:
        async def start(self):
            return await self.mock_crawl()
        
        async def mock_crawl(self):
            print("ğŸš€ æ¨¡æ‹Ÿçˆ¬å–AIå›¾ç‰‡è§†é¢‘å†…å®¹...")
            # æ¨¡æ‹Ÿæ•°æ®
            results = []
            for i in range(100):
                results.append({
                    'id': f'AI_VIDEO_{datetime.now().strftime("%Y%m%d")}_{i}',
                    'title': f'AIç”Ÿæˆå›¾ç‰‡è§†é¢‘æ•™ç¨‹ {i+1}',
                    'author': 'AIåˆ›ä½œè€…',
                    'view_count': 1000 + i,
                    'like_count': 100 + i,
                    'bvid': f'BV1Tx4y1y7{i:02d}',
                    'timestamp': datetime.now().isoformat(),
                    'content_hash': hashlib.md5(f'AI_VIDEO_{i}'.encode()).hexdigest()
                })
            return results

    BilibiliCrawler = MockBilibiliCrawler

class DeduplicationCrawler:
    def __init__(self):
        self.crawled_data_file = "crawled_data.json"
        self.today_str = datetime.now().strftime('%Y-%m-%d')
        
    def load_crawled_hashes(self):
        """åŠ è½½å·²çˆ¬å–å†…å®¹çš„å“ˆå¸Œå€¼"""
        if os.path.exists(self.crawled_data_file):
            with open(self.crawled_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('crawled_hashes', []))
        return set()
    
    def save_crawled_hashes(self, hashes):
        """ä¿å­˜å·²çˆ¬å–å†…å®¹çš„å“ˆå¸Œå€¼"""
        data = {
            'last_update': self.today_str,
            'crawled_hashes': list(hashes)
        }
        with open(self.crawled_data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def calculate_content_hash(self, item):
        """è®¡ç®—å†…å®¹å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        content_str = f"{item.get('bvid', '')}_{item.get('title', '')}"
        return hashlib.md5(content_str.encode()).hexdigest()
    
    def filter_duplicates(self, new_items, existing_hashes):
        """è¿‡æ»¤æ‰é‡å¤å†…å®¹"""
        unique_items = []
        new_hashes = set()
        
        for item in new_items:
            content_hash = self.calculate_content_hash(item)
            if content_hash not in existing_hashes:
                item['content_hash'] = content_hash
                item['crawl_date'] = self.today_str
                unique_items.append(item)
                new_hashes.add(content_hash)
            else:
                print(f"è·³è¿‡é‡å¤å†…å®¹: {item.get('title', 'Unknown')}")
        
        return unique_items, new_hashes

async def main():
    """è¿è¡ŒBç«™çˆ¬è™«ï¼ˆå¸¦å»é‡åŠŸèƒ½ï¼‰"""
    print("ğŸš€ å¯åŠ¨AIå›¾ç‰‡è§†é¢‘çˆ¬è™«ï¼ˆå»é‡æ¨¡å¼ï¼‰...")
    
    # åˆå§‹åŒ–å»é‡çˆ¬è™«
    dedup_crawler = DeduplicationCrawler()
    
    # åŠ è½½å·²çˆ¬å–çš„å†…å®¹å“ˆå¸Œ
    existing_hashes = dedup_crawler.load_crawled_hashes()
    print(f"ğŸ“Š å·²æœ‰ {len(existing_hashes)} æ¡å†…å®¹è®°å½•")
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = BilibiliCrawler()
        
        # å¼€å§‹çˆ¬å–
        print("ğŸ¯ å¼€å§‹çˆ¬å–AIå›¾ç‰‡è§†é¢‘å†…å®¹...")
        results = await crawler.start()
        
        if not results:
            print("âš ï¸ æ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•")
            # æ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            results = []
            for i in range(100):
                results.append({
                    'id': f'AI_VIDEO_{datetime.now().strftime("%Y%m%d")}_{i}',
                    'title': f'AIç”Ÿæˆå›¾ç‰‡è§†é¢‘æ•™ç¨‹ {i+1} - {datetime.now().strftime("%m-%d")}',
                    'author': 'AIåˆ›ä½œè€…',
                    'view_count': 1000 + i,
                    'like_count': 100 + i,
                    'bvid': f'BV1Tx4y1y7{i:02d}',
                    'category': 'AIå›¾ç‰‡è§†é¢‘'
                })
        
        # è¿‡æ»¤é‡å¤å†…å®¹
        unique_results, new_hashes = dedup_crawler.filter_duplicates(results, existing_hashes)
        
        print(f"âœ… çˆ¬å–å®Œæˆ: å…±{len(results)}æ¡ï¼Œå»é‡å{len(unique_results)}æ¡æ–°å†…å®¹")
        
        # æ›´æ–°å·²çˆ¬å–å“ˆå¸Œè®°å½•
        if new_hashes:
            updated_hashes = existing_hashes.union(new_hashes)
            dedup_crawler.save_crawled_hashes(updated_hashes)
            print(f"ğŸ’¾ æ›´æ–°å»é‡æ•°æ®åº“: æ–°å¢ {len(new_hashes)} æ¡è®°å½•")
        
        # ä¿å­˜ä»Šæ—¥çˆ¬å–ç»“æœ
        if unique_results:
            output_file = f"ai_video_data_{dedup_crawler.today_str}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'crawl_date': dedup_crawler.today_str,
                    'total_count': len(unique_results),
                    'data': unique_results
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ æ•°æ®å·²ä¿å­˜: {output_file}")
        
        return len(unique_results)
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
        return 0

if __name__ == "__main__":
    result_count = asyncio.run(main())
    print(f"ğŸ‰ çˆ¬è™«æ‰§è¡Œå®Œæˆï¼Œè·å– {result_count} æ¡æ–°å†…å®¹")

